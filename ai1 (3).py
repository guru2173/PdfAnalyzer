# -*- coding: utf-8 -*-
"""AI1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o_IADrzWSEe1XKEV4zhxBLR9A3zto5Ow
"""

!pip install torch transformers gradio pypdf huggingface_hub faiss-cpu rank_bm25 sentence-transformers

# ‚úÖ Authenticate Hugging Face Account
from huggingface_hub import notebook_login
notebook_login()

# ‚úÖ Install Required Libraries
#!pip install torch transformers gradio pypdf huggingface_hub faiss-cpu rank_bm25 sentence-transformers

# ‚úÖ Authenticate Hugging Face Account
#from huggingface_hub import notebook_login
#notebook_login()  # This will ask you to enter your Hugging Face token

import torch
import faiss
import pypdf
import gradio as gr
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi

# ‚úÖ Check GPU Availability
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ‚úÖ Load T5-Base Model
qa_pipeline = pipeline("text2text-generation", model="t5-base", device=0)

# ‚úÖ Load Sentence Transformer for Dense Retrieval
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# ‚úÖ Function to Extract Text from PDF
def extract_text_from_pdf(pdf_path):
    pdf_reader = pypdf.PdfReader(pdf_path)
    text = " ".join([page.extract_text() for page in pdf_reader.pages if page.extract_text()])
    return text

# ‚úÖ Upload & Process PDF
def process_pdf(pdf_file):
    global book_text, chunks, bm25, index

    book_text = extract_text_from_pdf(pdf_file.name)
    chunks = [book_text[i:i+512] for i in range(0, len(book_text), 512)]

    # ‚úÖ BM25 Sparse Retrieval
    bm25 = BM25Okapi([chunk.split() for chunk in chunks])

    # ‚úÖ FAISS Dense Retrieval
    embeddings = embedding_model.encode(chunks, convert_to_numpy=True)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)

    return "‚úÖ Book uploaded and processed successfully!"

# üîç Hybrid RAG: Retrieve Relevant Passages
def retrieve_best_passages(query, top_k=5):
    query_embed = embedding_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embed, top_k)
    best_chunks = [chunks[i] for i in indices[0]]

    # ‚úÖ BM25 Sparse Re-Ranking
    best_chunks = sorted(best_chunks, key=lambda x: bm25.get_scores(query.split())[chunks.index(x)], reverse=True)

    return best_chunks[:top_k]

# ‚úÖ Generate Answers Based on Question Type (Improved Prompt Engineering)
def answer_question(question):
    best_passages = retrieve_best_passages(question, top_k=5)
    context = " ".join(best_passages)

    # üî• Advanced Prompt Engineering: Structured Question Handling
    if "what" in question.lower():
        prompt = f"Provide a clear and precise definition based on the following content:\n\n{context}\n\nDefinition:"
    elif "explain" in question.lower():
        prompt = f"Explain the following concept in detail, including key aspects:\n\n{context}\n\nExplanation:"
    elif "summarize" in question.lower():
        prompt = f"Summarize the key points from the following text:\n\n{context}\n\nSummary:"
    elif "types of" in question.lower() or "list" in question.lower():
        prompt = f"List and describe the different types mentioned in the following content:\n\n{context}\n\nTypes:"
    elif "steps" in question.lower() or "process" in question.lower():
        prompt = f"Describe the step-by-step process based on the content below:\n\n{context}\n\nSteps:"
    else:
        prompt = f"Answer the following question based strictly on the provided context:\n\n{context}\n\nQuestion: {question}\n\nAnswer:"

    response = qa_pipeline(prompt, max_length=500, do_sample=True)
    return response[0]['generated_text']

# ‚úÖ Deploy as a Web App (Gradio)
with gr.Blocks() as iface:
    gr.Markdown("# üìö Book-Based Q&A System with Hybrid RAG & Advanced Prompt Engineering\nUpload a PDF and ask structured questions.")

    pdf_upload = gr.File(label="üìÇ Upload your book (PDF)")
    upload_button = gr.Button("Process Book")
    output_text = gr.Textbox(label="üìå Status", interactive=False)

    question_input = gr.Textbox(label="üîç Ask a Question")
    answer_output = gr.Textbox(label="üìñ Answer", interactive=False)
    ask_button = gr.Button("Get Answer")

    upload_button.click(process_pdf, inputs=pdf_upload, outputs=output_text)
    ask_button.click(answer_question, inputs=question_input, outputs=answer_output)

iface.launch(share=True)